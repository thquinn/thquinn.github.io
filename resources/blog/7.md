# MCTS & Magic: the Gathering
##### January ??, 2021

<img src="resources/images/blog/7-0.png" style="width: 50%;" />

**TL;DR:** the first 90% this post gets into the weeds about adapting Monte Carlo tree search for Magic. For those less interested in the technical side, [the results are down here.](#tldr) 

I couldn't stay away! While my [previous post](blog.html?post=6) got some fun results and nice feedback, I knew I could do better. I wanted to analyze a more powerful and complex combo using techniques from this century, creating a deck that could dominate its format and an AI able to [goldfish](https://mtg.fandom.com/wiki/Goldfishing) it at a superhuman level. A high bar — I had a lot to figure out.

### The deck

![Image](resources/images/blog/7-1.png)

This time I went for [Jeskai Ascendancy Combo](https://pennydreadfulmagic.com/archetypes/Jeskai%20Ascendancy/), sticking with the Penny Dreadful format since it's awesome and I'm a cheapass. This combo uses the titular Jeskai Ascendancy and cheap cantrips to loot away and unearth Fatestitchers, ultimately generating enough mana, card selection, and power to attack for the win as early as turn 4. I opted for the non-green, non-<auto-card>Pyromancer Ascension</auto-card> version to keep the board and mana simple. The current season of Penny Dreadful has a lot to offer the deck: <auto-card>Brainstorm</auto-card>, <auto-card>Frantic Search</auto-card>, <auto-card>Treasure Cruise</auto-card>... Modern, eat your heart out! Another thing that drew me to the combo: since cantrips are effectively a part of the combo, there are tons of cards that could potentially slot in. Whereas my analysis of Near-Death Experience Combo had me mostly fiddling with the manabase, the interchangeability here gives us a nice big space to explore.

### Monte-Carlo tree search

It might be reasonable to try [expectimax](https://en.wikipedia.org/wiki/Expectiminimax). On the other hand:

* the branching factor of this deck can spike up around 100
* using expectimax for a solitaire game precludes the possibility of pruning
* I don't want to write a heuristic that accounts for the ~30 cards I wanted to test, and
* even if I could, I don't want to sully the results with my assumptions

Plus, I had set out to use more modern methodology, so it was gonna have to be [Monte Carlo tree search](https://en.wikipedia.org/wiki/Monte_Carlo_tree_search). Other people have written [better explanations](https://medium.com/@quasimik/monte-carlo-tree-search-applied-to-letterpress-34f41c86e238) than I can; I'd encourage you to check one out if you're unfamiliar.

### Chance nodes

We have to make two modifications from the standard implementation of MCTS. The first one is actually a simplification: since goldfish Magic is a one-player game, we don't have to keep track of which player is to move in which node and negate rollout results accordingly.

The second is a problem: Magic is a stochastic game. Random events (the cards you draw, for example) are outside of the player's control, which kind of ruins the typical selection process of MCTS. Poking around the internet for solutions, I came across the phrase "chance node," and being the wheel-reinventing type, I immediately stopped researching and got down to implementation.

![Image](resources/images/blog/7-2.png)

In this scheme, any action with a deterministic outcome (playing a land, putting a spell on the stack) results in a normal MCTS node, which I call "choice nodes." During the selection phase, we use typical confidence bound calculations to choose which child to explore. On the other hand, any action with a random outcome (letting a cantrip resolve, ending the turn and drawing a card) results in a chance node. When we encounter one of these during selection, we query the game state object for an outcome rather than making a choice ourselves.

Because we want to experience the full gamut of possibilities at each chance node, it's important that the game state remain as unresolved as possible. That is to say, we don't want to initialize a state by creating an array of 60 cards and shuffling them; if we did, we would always draw the same cards in the same order during rollouts! Instead, I represent the deck as an unordered list of card quantities, more of a grab bag than a stack of cards.

<pre><code>Stack&lt;Card&gt; deck; // WRONG: we don't want our cards in a known order
int[] deckQuantities; // RIGHT: indexed by Card enum value, in undetermined order</code></pre>

Letting randomness dictate our MCTS selection introduces one more minor problem. We've reached a chance node and drawn a card: how do we know if we've seen our now random state before? We didn't choose a child node, after all. We could look at all of the chance node's children and compare with the state we've found ourselves in, but that sounds potentially time intensive; some chance nodes can have hundreds of children (drawing 3 cards off <auto-card>Brainstorm</auto-card>, for instance). Instead, I have my ExecuteMove function return an ID that uniquely represents a random event, if one occurred. Indexing chance-node children by this ID lets us dive down the tree nice and fast.

<img src="resources/images/blog/7-3.png" style="width: 75%;" />

In addition to this ID, ExecuteMove also returns the probability of the event that just occurred, so we can track the cumulative probability of our selection as it progresses and use it to make optimizations. In the example above, after encountering three chance nodes along a path through our tree, we find ourselves about to expand, creating a node that has a cumulative probability of 0.001%: one in a hundred thousand. Even if we found great success here, and focused the remainder of our search trying to get back to this node and exploit it further, we may never see it again. Is it really worth the memory to create this child node and gather statistics on it, or should we just attribute those statistics to its parent? We may want to simulate first, since the success or failure of the rollout will affect our chances of returning just as the cumulative probability does. We would then, in effect, have two probability thresholds for node creation.

(In practice, I was running games at 100K rollouts max, and so didn't have much use for this tradeoff. In tests searching for closer-to-optimal play, I'd expect it would be more useful.)

### Reward distributions

So we make it to the simulation phase, perform a random rollout, and through a stroke of luck manage to get our Fatestitchers up to a combined 20 power — that's a win, baby! ([Check out the source code](https://github.com/thquinn/JeskaiAscendancyMCTS/search?q=simplification) for a rundown of the simplifications made.) What value do we propagate up the tree? In the standard implementation of MCTS, we would just increment each ancestor's win count by 1, but here's another curveball from goldfish Magic: we're trying to win on the earliest turn possible. A turn 8 win is simply not as good as a turn 4 win; in fact, in a non-goldfish game, it's not likely to be a win at all! How do we further modify MCTS to make it care about when we win?

To our great luck, MCTS is kinda already built for variable, even continuous outcomes. The so-called ["multi-armed bandit" problem](https://en.wikipedia.org/wiki/Multi-armed_bandit) that serves as the basis of MCTS isn't an inherently all-or-nothing proposition in the first place. We just need to backpropagate a higher reward for earlier wins. Easy enough.

...Or is it? It is not. Whatever deck and agent we come up with are going to have a win-turn distribution that look something like these:

<img src="resources/images/blog/7-4.png" style="width: 75%;" />

These kinds of distributions are not necessarily transitive; like choosing between rock, paper, and scissors, there is no "best" distribution. (Check out this fun [Numberphile video on nontransitive dice](https://www.youtube.com/watch?v=zzKGnuvX6IQ).) We may weight our rewards heavily towards the turn 4 win, causing our AI to aggressively mulligan away even hands that can likely kill on turn 5, or we may weight them more evenly to improve our chances of comboing off before turn 8... but no matter what we pick, there may be a strategy that exploits ours. It's hard to know for sure, since the win-turn distribution that we push our AI towards and the one it's actually able to achieve are not going to be the same.

Even though our analysis is single player, the ideal set of rewards would probably be reflective of the metagame of the format. There would be separate reward sets for being on the play or on the draw, and each reward would represent a goldfish deck's chances of winning against the "average" opponent on that particular turn. And of course, an AI built to play against an actual opponent would use the opponent's known cards to guess which deck they're playing, and simulate their deck to determine their win-turn distribution... to avoid this rabbit hole, I just closed my eyes and typed an array:

<pre><p style="text-align: center;">[0, 0, 0, 0, 1, 0.75, 0.5, 0.25]</p></pre>

Which is to say: 100% chance to win if we combo on turn 4, 75% on turn 5, 50% on turn 6, 25% on turn 7, no chance to win if it takes us longer than that. Given the generally durdley nature of Penny Dreadful, I figure it's not so bad a guess.

### Mulligan decisions

With this concept of "expected reward," we can handle mulligans a little more intelligently. We could certainly just include mulligans in our search, making it the very first choice node of a new game:

<img src="resources/images/blog/7-5.png" style="width: 85%;" />

The mulligan subtree is immensely ugly, extending just about forever breadthwise. We're talking a branching factor on the order of tens of millions: the statistical power of MCTS has fallen to just about zero. And that's not even the worst part: we're going to have to do this exact same work at the beginning of every game! Yuck.

Instead, when given the opportunity to mulligan, we do a single, deep MCTS run on our opening hand, assuming a keep. We look at the average expected reward of the move we would make — our most-played ("robust") child — and compare it to precomputed equivalents for hands of one less card, averaged out over many tests. This way, we can compare our hand against a more accurate representation of the average mulligan, and we get to reuse the entire tree generated for the mulligan decision to instantly make our first move and get a headstart towards the next few.

This does mean that if we change anything — the decklist, the reward array, the number of rollouts we devote to mulligan decisions, or the way we perform random rollouts — we must recompute our mulligan thresholds. Spoiler: this will not work when we're trying to tune a deck. Oh, and speaking of "the way we perform random rollouts":

### Simulation quality

My initial implementation of this stochastic MCTS used the light rollout: choosing at random from among all legal moves, repeat until a terminal state is reached. And it worked! I was floored when my first runs were actually able to win:

<pre><code>rollouts = 100
	Win rate before turn 12: 87.0%. Average win turn: 7.53.
rollouts = 1000
	Win rate before turn 12: 99.1%. Average win turn: 6.59.
rollouts = 10000
	Win rate before turn 12: 99.5%. Average win turn: 5.83.
rollouts = 100000 (parallelism: 8, run time: ~2 hours)
	Win rate before turn 12: 100.0%. Average win turn: 5.57.</code></pre>

So it was quite a while before I experimented with rollouts that were even a little heavy. Rather than developing any kind of evaluation function, though, I just added some rules of thumb, some general and some domain specific:

* shuffle the output of GetMoves() for more uniform searching/rollouts
* top one card at a time with Brainstorm instead of composing into a single layer of nodes
* always unearth Fatestitcher immediately with a Jeskai Ascendancy in play, and never otherwise
* never end the turn unless there are no other moves

That last one is awful suspicious, but let's take a look at the performance afterwards:

<pre><code>rollouts = 1000
	Average reward: 0.731 over 2000 trials. Win rate before turn 8: 97.7%. Average win turn: 5.01.
rollouts = 10000
	Average reward: 0.75 over 1000 trials. Win rate before turn 8: 98.0%. Average win turn: 4.93.</code></pre>

Whoa. Working with the same deck, our new expected win turn at 1K rollouts is now over half a turn sooner than our previous performance at 100K, and runs pretty much 100 times as fast. It turns out that even rough rules of thumb, like never ending the turn unless you have to, can dramatically improve the quality of your rollouts, in turn dramatically improving performance.

It makes sense: how many near-win light rollouts are ruined when, with 15 cards in hand, 10 mana in the pool, and three 6/6 <auto-card>Fatestitcher</auto-card>s in play, the random number generator decides to simply end the turn? It's a wonder it was able to win at all, frankly (though the winrate of a totally random agent was somehow over 5% in the two million games I tested). Note that this doesn't mean that the agent will now always cast <auto-card>Gitaxian Probe</auto-card> on turn 1: the recursive nature of MCTS is perfectly capable of determining that it's best to save cantrips in certain situations. Your rollout strategy will not necessarily become your agent's overall strategy.

### Decklist search

We now have an agent that averages a turn-5 win with our starting configuration of cards:

<div class="decklist">1x <auto-card>Plains</auto-card>
12x <auto-card>Island</auto-card>
1x <auto-card>Mountain</auto-card>
4x <auto-card>Evolving Wilds</auto-card>
4x <auto-card>Mystic Monastery</auto-card>
4x <auto-card>Brainstorm</auto-card>
2x <auto-card>Cerulean Wisps</auto-card>
4x <auto-card>Fatestitcher</auto-card>
4x <auto-card>Frantic Inventory</auto-card>
4x <auto-card>Frantic Search</auto-card>
4x <auto-card>Gitaxian Probe</auto-card>
4x <auto-card>Jeskai Ascendancy</auto-card>
2x <auto-card>Obsessive Search</auto-card>
4x <auto-card>Opt</auto-card>
4x <auto-card>Ponder</auto-card>
2x <auto-card>Treasure Cruise</auto-card></div>

I implemented a few other lands: <auto-card>Izzet Boilerworks</auto-card>, <auto-card>Meandering River</auto-card>, <auto-card>Highland Lake</auto-card>, and <auto-card>Vivid Creek</auto-card>. I also implemented a bunch of alternative spells cribbed from 5-0 league decks (and a few from my jank daydreams): <auto-card>Desperate Ravings</auto-card>, <auto-card>Ideas Unbound</auto-card>, <auto-card>Izzet Charm</auto-card>, <auto-card>Magmatic Insight</auto-card>, <auto-card>Omen of the Sea</auto-card>, <auto-card>Think Twice</auto-card>, <auto-card>To Arms!</auto-card>, <auto-card>Vision Skeins</auto-card>, and <auto-card>Witching Well</auto-card>. 29 cards in all... it didn't seem like much, but a large enough space to search. And how to search this large-enough space? I mean... I guess if MCTS worked once, it could work again?

<img src="resources/images/blog/7-6.png" style="width: 66%;" />

In this toy example, we start with a deck consisting of 30 copies of <auto-card>Forest</auto-card> and 30 <auto-card>Grizzly Bears</auto-card>. The root's children compose the first *removal layer*, each node corresponding to one card being removed from the root deck (marked as -F and -GB) or no card being removed from the deck (the first decklist node, 30x Forest and 30x Grizzly Bears). The children of the first removal layer compose the first *addition layer*, each node corresponding to one card being added to the root deck (+F and +GB).

After a certain number of rollouts, a change (in our example: -1 Forest, -1 Grizzly Bears, or no changes) is selected. If "no changes" is deemed the best change, the deck is finalized! Otherwise, a new run begins with the removal node as the root. Its children will be the first addition layer, and restrictions from previous runs (which cards are still eligible to be added/removed) are maintained.

Rollouts are only performed from decklist nodes, and the selection algorithm is modified such that it always either ends in a decklist node or in the creation of a decklist node. These are heavy rollouts: they consist of an entire game being played with that deck, and the win-turn reward from that game is backpropagated. Another important property of the tree: once a copy of a card has been removed in a given subtree, no copies of that card can be added within that subtree, and vice versa. This does introduce opportunities to get stuck in local maxima, the tradeoff being a gradual reduction in the branching factor and preventing the search from going in circles.

<img src="resources/images/blog/7-7.png" style="width: 60%;" />

That's not to say there aren't transpositions. The tree above is 10 rollouts into a search between Forests, Grizzly Bears, and <auto-card>Giant Growth</auto-card>s, and has already encountered a transposition (outlined in red). There are existing methods to deal with transpositions in MCTS, but they didn't make up a significant portion of my test trees, so I just left them be.

I can think of a potential downside to searching in this order (removing a card, then adding a card). If a card exists that would be extremely beneficial to add, but no obvious best candidate to remove, the benefit of adding the card would be spread across all children in the removal layer except the decklist node. Of course, the equivalent and opposite problem would exist if we were to add first, then remove: an enhancement like [RAVE](https://www.cs.utexas.edu/~pstone/Courses/394Rspring13/resources/mcrave.pdf) might help us with this, or some other method of sharing partial statistics across similar subtrees.

### Early stopping

We do run into a related problem: the unchanged decklist node, the one that's a direct child of the root... it's too tempting. While other children are expanded and testing with outlandish, unworkable changes, the unchanged decklist remains, well, unchanged, consistently producing solid results while the other prospects thrash. Like any gambler, MCTS (my implementation, anyway) can't resist a sure bet. Trying to fix the problem with a higher exploration parameter drives the reward thresholds of the other children lower as they fail to exploit their most promising subtrees, and a lower exploration parameter only serves to further fixate the search on the unchanged decklist. And so we turn to a cruder solution:

![Image](resources/images/blog/7-8.png)

We force the decklist search to make a change. This has an obvious problem: how do we know when to stop? We don't — not in the context of the decklist search. The (crude, again) solution is to test each iteration over many trials. We stop the decklist search once the average begins to drop as it runs out of beneficial swaps to make and is forced to choose among the bad ones.

### Parallelization

We have the luxury of not having to worry about parallelizing both the gameplay search and decklist search, and the high branching factor and super-heavy rollouts of our decklist search make it seem particularly suited for full-on [tree parallelization](https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.159.4373&rep=rep1&type=pdf). I found a [lock-free implementation](http://liacs.leidenuniv.nl/~plaata1/papers/paper_ICAART18.pdf), but because the vast majority of the decklist search is spent in rollouts rather than tree operations, locks shouldn't slow us down much. I went with the standard implementation, with the generally recommended "virtual loss" enhancement: incrementing simulation count on the way down the tree and reward totals on the way back up. I've seen evidence that virtual losses can be detrimental to search quality, and I would love to test that... but these decklist runs take so long on my machine that it would take years to get a significant sample size. Time to roll the dice.

<a id="tldr"></a>
### The results

I started with a decklist slightly modified from the one above:

<div class="decklist">1x <auto-card>Plains</auto-card>
12x <auto-card>Island</auto-card>
1x <auto-card>Mountain</auto-card>
4x <auto-card>Evolving Wilds</auto-card>
4x <auto-card>Mystic Monastery</auto-card>
4x <auto-card>Brainstorm</auto-card>
2x <auto-card>Cerulean Wisps</auto-card>
4x <auto-card>Fatestitcher</auto-card>
2x <auto-card>Frantic Inventory</auto-card>
4x <auto-card>Frantic Search</auto-card>
4x <auto-card>Gitaxian Probe</auto-card>
4x <auto-card>Jeskai Ascendancy</auto-card>
2x <auto-card>Obsessive Search</auto-card>
2x <auto-card>Omen of the Sea</auto-card>
4x <auto-card>Opt</auto-card>
4x <auto-card>Ponder</auto-card>
2x <auto-card>Treasure Cruise</auto-card></div>

We can't expect much depth from our branching factor ~30 search, so I made the choice to start at 2 copies of Frantic Inventory. My intuition is that 0 or 4 are local maxima too strong to overcome with the amount of compute I have, so I start right in between them. I also made a few hypotheses:

* The number of Frantic Inventory will settle at either 0 or 4, probably 0.
* We'll cut Islands and introduce more fixing lands.
* We won't lose the Plains or Mountain, otherwise we'd have no Evolving Wilds targets.
* Since bouncelands were so successful in my last analysis, maybe two <auto-card>Izzet Boilerworks</auto-card>.
* Gitaxian Probe will stay at 4 copies.
* We'll pick up at least one <auto-card>Ideas Unbound</auto-card>.

I locked the number of Fatestitcher and Jeskai Ascendancy to 4, which one might consider cheating — ah well — and set it to run games at 1K rollouts, searching for 8 hours (70-80K total games played) between each ply (i.e. adding *or* removing a card). Along the way, decks were evaluated for this graph by averaging 20K games played at 1K rollouts. Here are the results of *over three weeks* of compute on 8 threads:

![Image](resources/images/blog/7-9.png)

The best decklist was found 13 swaps (26 ply) into our decklist search, after losing the third Evolving Wilds and picking up a third Vivid Creek, and is:

<div class="decklist">3x <auto-card>Plains</auto-card>
7x <auto-card>Island</auto-card>
3x <auto-card>Mountain</auto-card>
1x <auto-card>Evolving Wilds</auto-card>
4x <auto-card>Mystic Monastery</auto-card>
3x <auto-card>Vivid Creek</auto-card>
4x <auto-card>Brainstorm</auto-card>
3x <auto-card>Cerulean Wisps</auto-card>
4x <auto-card>Fatestitcher</auto-card>
4x <auto-card>Frantic Search</auto-card>
4x <auto-card>Gitaxian Probe</auto-card>
1x <auto-card>Ideas Unbound</auto-card>
4x <auto-card>Jeskai Ascendancy</auto-card>
4x <auto-card>Obsessive Search</auto-card>
3x <auto-card>Opt</auto-card>
4x <auto-card>Ponder</auto-card>
3x <auto-card>Treasure Cruise</auto-card>
1x <auto-card>Vision Skeins</auto-card></div>

My hypotheses did okay, much better than [last time](blog.html?post=6) anyway. We did indeed cut to 0 Frantic Inventory pretty quickly: a good sign. We cut on Islands for other colors, but I was surprised to see Evolving Wilds cut and no tapped duals introduced. No bouncelands, either: presumably too slow. No time for value on this combo train.

### In conclusion

Let's do a final run with our player agent and see just how fast PD Jeskai Ascendancy Combo can get.

<pre><code>rollouts = 1000
	Average reward: 0.784 over 20K trials. Win rate before T8: 97.6%. Average win turn: 4.79.
rollouts = 10000
	Average reward: 
rollouts = 100000
	Average reward: </code></pre>