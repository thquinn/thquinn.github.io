# MCTS & Magic: the Gathering
##### December ??, 2020

<img src="resources/images/blog/7-0.png" style="width: 50%;" />

**TL;DR:** the first half of this post gets into the weeds about adapting Monte Carlo tree search for Magic. For those less interested in the technical side, [the results are down here.](#tldr) 

I couldn't stay away! While my [previous post](blog.html?post=6) got some fun results and nice feedback, I knew I could do better. I wanted to analyze a more powerful and complex combo using techniques from this century, creating a deck that could dominate its format and an AI able to [goldfish](https://mtg.fandom.com/wiki/Goldfishing) it at a superhuman level. A high bar — I had a lot to figure out.

### The deck

![Image](resources/images/blog/7-1.png)

This time I went for [Jeskai Ascendancy Combo](https://pennydreadfulmagic.com/archetypes/Jeskai%20Ascendancy/), sticking with the Penny Dreadful format since it's awesome and I'm a cheapass. This combo uses the titular Jeskai Ascendancy and cheap cantrips to loot away and unearth Fatestitchers, ultimately generating enough mana, card selection, and power to attack for the win as early as turn 4. I opted for the non-green, non-<auto-card>Pyromancer Ascension</auto-card> version to keep the board and mana simple. The current season of Penny Dreadful has a lot to offer the deck: <auto-card>Brainstorm</auto-card>, <auto-card>Frantic Search</auto-card>, <auto-card>Treasure Cruise</auto-card>... Modern, eat your heart out! Another thing that drew me to the combo: since cantrips are effectively a part of the combo, there are tons of cards that could potentially slot in. Whereas my analysis of Near-Death Experience Combo had me mostly fiddling with the manabase, the interchangeability here gives us a nice big space to explore.

### Monte-Carlo tree search

It might be reasonable to try [expectimax](https://en.wikipedia.org/wiki/Expectiminimax). On the other hand:

* the branching factor can spike up around 100
* using it for a solitaire game precludes the possibility of pruning
* it's annoying to write a heuristic that accounts for the ~30 cards I wanted to test, and
* even if I could, I don't want to sully the results with my assumptions

Plus, I had set out to use more modern methodology, so it was gonna have to be [Monte Carlo tree search](https://en.wikipedia.org/wiki/Monte_Carlo_tree_search). Other people have written [better explanations](https://medium.com/@quasimik/monte-carlo-tree-search-applied-to-letterpress-34f41c86e238) than I can; I'd encourage you to check one out if you're unfamiliar.

### Chance nodes

We have to make two modifications from the standard implementation of MCTS. The first one is actually a simplification: since goldfish Magic is a one-player game, we don't have to keep track of which player is to move in which node and negate rollout results accordingly.

The second is a problem: Magic is a stochastic game. Random events (the cards you draw, for example) are outside of the player's control, which kind of ruins the typical selection process of MCTS. Poking around the internet for solutions, I came across the phrase "chance node," and being the wheel-reinventing type, I immediately stopped researching and got to implementation.

![Image](resources/images/blog/7-2.png)

In this scheme, any action with a deterministic outcome (playing a land, putting a spell on the stack) results in a normal MCTS node, which I call "choice nodes." During the selection phase, we use typical confidence bound calculations to choose which child to explore. On the other hand, any action with a random outcome (letting a cantrip resolve; ending the turn, thereby drawing a card) results in a chance node. When we encounter one of these during selection, we query the game state object for an outcome rather than making a choice ourselves.

Because we want to experience the full gamut of possibilities at each chance node, it's important that the game state remain as unresolved as possible. That is to say, we don't want to initialize a state by creating an array of 60 cards and shuffling them; if we did, we would always draw the same cards in the same order during rollouts! Instead, I represent the deck as an unordered list of card quantities, more of a grab bag than a stack of cards.

Letting randomness dictate our MCTS selection introduces one more minor problem. We've reached a chance node and drawn a card: how do we know if we've seen our now random state before? We didn't choose a child node, after all. We could look at all of the chance node's children and compare with the state we've found ourselves in, but that sounds potentially time intensive; some chance nodes can have hundreds of children (drawing 3 cards off <auto-card>Brainstorm</auto-card>, for instance). Instead, I have my ExecuteMove function return an ID that uniquely represents a random event, if one occurred. Indexing chance-node children by this ID lets us dive down the tree nice and fast.

<img src="resources/images/blog/7-3.png" style="width: 75%;" />

In addition to this ID, ExecuteMove also returns the probability of the event that just occurred, so we can track the cumulative probability of our selection as it progresses and make some optimizations. In the example above, after encountering three chance nodes along a path through our tree, we find ourselves about to expand, creating a node that has a cumulative probability of 0.001%: one in a hundred thousand. Even if we found great success here, and focused the remainder of our search trying to get back to this node and exploit it further, we may never see it again. Is it really worth the memory to create this child node and gather statistics on it, or should we just attribute those statistics to its parent? We may want to simulate first, since the success or failure of the rollout will affect our chances of returning just as the cumulative probability does. We would then, in effect, have two probability thresholds for node creation.

### Reward distributions

So we make it to the simulation phase, perform a random playout, and through a stroke of luck manage to get our Fatestitchers up to a combined 20 power — that's a win, baby! ([Check out the source code](https://github.com/thquinn/JeskaiAscendancyMCTS/search?q=simplification) for a rundown of the simplifications made.) What value do we propagate up the tree? In MCTS implementations it would be 1, but here's another curveball from goldfish Magic: we're trying to win on the earliest turn possible. A turn 8 win is simply not as good as a turn 4 win; in fact, in an actual non-goldfish game, it's not likely to be a win at all! How do we even further modify MCTS to care about when we win?

To our great luck, MCTS is kinda already built for variable, even continuous outcomes. The so-called ["multi-armed bandit" problem](https://en.wikipedia.org/wiki/Multi-armed_bandit) that serves as the basis of MCTS isn't an inherently all-or-nothing proposition in the first place. We just need to backpropagate a higher reward for earlier wins. Easy enough.

...Or is it? It is not. Whatever deck and agent we come up with are going to have a win-turn distribution that look something like these:

<img src="resources/images/blog/7-4.png" style="width: 75%;" />

These kinds of distributions are not necessarily transitive; like choosing between rock, paper, and scissors, there is no "best" distribution. (Check out this fun [Numberphile video on nontransitive dice](https://www.youtube.com/watch?v=zzKGnuvX6IQ).) We may weight our rewards heavily towards the turn 4 win, causing our AI to aggressively mulligan away even hands that can likely kill on turn 5, or we may weight them more evenly to improve our chances of comboing off before turn 8... but no matter what we pick, there may be a strategy that exploits ours.

Even though our analysis is single player, the ideal set of rewards would probably be reflective of the metagame of the format. There would be separate reward sets for being on the play or on the draw, and each reward would represent a goldfish deck's chances of winning against the "average" opponent on that particular turn. And of course, an AI meant to play against an actual opponent would use the opponent's known cards to guess which deck they're playing, and simulate their deck to determine their win-turn distribution... to avoid this rabbit hole, I just closed my eyes and typed an array:

<pre><p style="text-align: center;">[0, 0, 0, 0, 1, 0.75, 0.5, 0.25]</p></pre>

Which is to say: 100% chance to win if we combo on turn 4, 75% on turn 5, 50% on turn 6, 25% on turn 7, no chance to win if it takes us longer than that. Given the generally durdley nature of Penny Dreadful, I figure it's not so bad a guess.

### Mulligan decisions

With this concept of "expected reward," we can handle mulligans a little more intelligently. We could certainly just include mulligans in our search, making it the very first choice node of a new game:

<img src="resources/images/blog/7-5.png" style="width: 85%;" />

The mulligan subtree is immensely ugly, extending just about forever breadthwise. We're talking a branching factor on the order of tens of millions: MCTS has long since become indistinguishable from depth-first search. And that's not even the worst part: we're going to have to do this exact same work at the beginning of every game! Yuck.

Instead, when given the opportunity to mulligan, we do a single, deep MCTS run on our opening hand, assuming a keep. We look at the average expected reward of the move we would make — our most-played ("robust") child — and compare it to precomputed equivalents for hands of one less card, averaged out over thousands of tests. This way, we can compare our hand against a more accurate representation of the average mulligan, and we get to reuse the entire tree generated for the mulligan decision to instantly make our first move and get a headstart towards the next few.

This does mean that if we change anything — the decklist, the reward array, the number of rollouts we devote to mulligan decisions, or the way we perform random rollouts — we must recompute our mulligan thresholds. Foreshadowing: this will not work when we're trying to tune a deck. Oh, and speaking of "the way we perform random rollouts":

### Simulation quality

My initial implementation of this stochastic MCTS used the light rollout: choosing at random from among all legal moves, repeat until a terminal state is reached. And it worked! I was floored when my first runs were actually able to win:

<pre><code>rollouts = 100
	Win rate before turn 12: 87.0%. Average win turn: 7.53.
rollouts = 1000
	Win rate before turn 12: 99.1%. Average win turn: 6.59.
rollouts = 10000
	Win rate before turn 12: 99.5%. Average win turn: 5.83.
rollouts = 100000 (parallelism: 8, run time: ~2 hours)
	Win rate before turn 12: 100.0%. Average win turn: 5.57.</code></pre>

So it was quite a while before I experimented with playouts that were even a little heavy. Rather than developing any kind of evaluation function, though, I just added some rules of thumb, some general and some domain specific:

* shuffle the output of GetMoves() for more uniform searching/rollouts
* top one card at a time with Brainstorm instead of composing into one layer of nodes
* always unearth Fatestitcher immediately with a Jeskai Ascendancy in play, and never otherwise
* never end the turn in rollouts unless forced

That last one is awful suspicious, but let's take a look at performance afterwards:

<pre><code>rollouts = 1000
	Average reward: 0.731 over 2000 trials. Win rate before turn 8: 97.7%. Average win turn: 5.01.
rollouts = 10000
	Average reward: 0.75 over 1000 trials. Win rate before turn 8: 98.0%. Average win turn: 4.93.</code></pre>

Whoa. Working with the same deck, our current performance at 1,000 rollouts is now more than a half turn faster than our previous performance at 100,000, and pretty much 100 times as fast. It turns out that even rough rules of thumb, like never ending the turn unless you have to, can dramatically improve the quality of your rollouts, in turn dramatically improving performance. And it doesn't mean that the agent will now always cast <auto-card>Gitaxian Probe</auto-card> on turn 1: the recursive nature of MCTS is perfectly capable of determining that it's best to save the Probe until it's needed. Your rollout strategy will not necessarily become your agent's overall strategy.

### Decklist search

We officially have an agent that averages a turn-5 win with our starting configuration, which is:

<auto-card-list preview src="resources/other/ascendancy_init.dec"></auto-card-list>

<a id="tldr"></a>
### The results