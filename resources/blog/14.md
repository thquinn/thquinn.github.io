# AlphaPush
##### January 18, 2025

A whole [four damn years ago](/blog.html?post=7), I wrote an "AI" that used Monte Carlo Tree Search to optimize and play a Magic: the Gathering deck. I put "AI" in quotes here because, as we now know in 2025, AI is neural networks. Ever since AlphaGo eked out a 4-1 victory against 9-dan professional Lee Sedol using MCTS and *deep learning*, I've wanted to replicate it, and now I have! On a much smaller scale, and stage.

## Push Fight

Go is hard, so I chose an easier target: a game called Push Fight. Please watch this informative video:

<iframe class="youtube" src="https://www.youtube.com/embed/bQZFF3fuV8M" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

The first player places their five pieces on their side of the board, the second player does the same, then it goes as Maizie says: "you move [between zero and] two times [as far as you want along contiguous spaces] and then push one square and you get a red hat and how you win a game is push off the edge." Only the square pieces can push, and the red hat is an anchor that locks that piece in place until your next turn.

<img src="resources/images/blog/14-0.png" style="width: 50%;" />

It's a simple game that, [despite being strongly solved](https://boardgamegeek.com/thread/2905868/push-fight-is-solved-plus-a-website-to-play-agains), comes a nasty little twist for wannabe AIs. Because each turn consists of multiple submoves — up to two moves, then a push — the branching factor is ~5,000, compared to the ~250 of Go and the ~35 of chess. And sure, you could call that cheating. After all, I could break a single chess move into multiple sequential decisions: choosing which piece to move and then where to move it, for example. You'd still be left with an average of 35 resultant game states to analyze from your opponent's perspective, and in Push Fight it really is a lot more. That means old-school AI techniques like minimax are right out.

## AlphaZero

*(I'll be assuming some familiarity with MCTS in this post, [here's a good explainer article](https://medium.com/@quasimik/monte-carlo-tree-search-applied-to-letterpress-34f41c86e238) if you need a refresher.)*

MCTS is *better* suited to a search space like this, but it's still too wide and too deep. The solution presented by AlphaGo, then refined by AlphaGo Zero and AlphaZero, is to replace the crappy, slow "rollouts" of MCTS with value estimates, and to guide search with "policy priors": estimates of how... let's say *interesting* each move is.

<img src="resources/images/blog/14-1.png" />

These value estimates and policy priors are where the neural networks come in. Using the results of vanilla MCTS playing against itself at some moderate number of playouts, you train a network to predict both the outcome of a game from a given position (value) and the proportion of visits spent exploring each move (policy). Then you again have MCTS play itself for many games, this time using the network's value and policy estimates to augment the search.

The question you're training the network to answer is, essentially: "If I asked a smarter version of you who is winning this game, what would they say?" It sounds kind of like the solution to [that classic riddle with the two doors](https://www.youtube.com/watch?v=uM3gK-OUvqE). And despite the apparent paradox, we can actually get an answer. The longer we give our agent to think, the smarter it gets, and using those examples to train our model makes it smarter even with no time to think. With this new network as our baseline, we can repeat the process to make it even smarter, etc.

## Results

Let's get right down to it: "AlphaPush" plays quite well! Over 10 rounds of self-play and training totaling 8.8 million board states, the network gained an estimated 1,015 Elo against its random initialization.

<img src="resources/images/blog/14-2.png" style="width: 66%;" />

## Topology and training

The input encoding is 160 elements long:

* bit 0: 1 if the current player is placing square pieces, otherwise 0.
    * (for the sake of simplicity, each player places its three pushers first, then its two round "pawns")
* bit 1: 1 if the current player is placing pawns, otherwise 0.
* bit 2: 1 if the current player is making game moves, otherwise 0.
* bit 3: how many optional moves does the current player have left? [0-2]
* bits 4-159: the six bitboards shown below:

<img src="resources/images/blog/14-3.png" />

The network runs the encoded input through 5 fully-connected hidden layers totaling 270K parameters, and has 807 outputs: a [-1, 1] value estimate and 806 policy logits corresponding to all possible moves (piece placements, slides, and pushes). Each hidden layer uses ReLU, batch normalization, and a configurable dropout during training (usually 20%). A tanh is applied to the value output, and during training, policy outputs are masked to only calculate cross-entropy loss for legal moves.

Self-play games were generated on Google Cloud's Compute Engine instances, which dropped every batch of 10K examples into a Google Cloud Storage bucket. I used [their free trial](https://cloud.google.com/free/docs/free-cloud-features) that gives you $300 of credit to play with and only ended up using $35. To avoid figuring out how to parallelize generation between cores, I went for the 1-vCPU "T2D" instances. The trial limitations required me to spin up 8 instances in each of 4 regions to hit the 32 vCPU max, which was kind of a pain, but whatever. I set the instances to use "[spot provisioning](https://cloud.google.com/compute/docs/instances/create-use-spot)," which slashed the cost by 80%. In practice, spot instances don't seem to get shut off almost ever.

## Learnings

#### Testing
Despite this being my fourth or fifth time implementing MCTS, it's a tricky algorithm, and each implementation has its own quirks and novel requirements. Bugs here impacted the quality of my training data right up until the end, and I was lucky not to run into any substantial issues with move generation or tensor handling. Despite my contentious relationship with test-driven development, I have to admit that it would absolutely have improved matters. No doubt I still have bugs that are hamstringing training/play.

#### MCTS
The power and extensibility of MCTS continues to amaze me. Even with a bug that backpropagated an inverted value for maybe *half* of all positions and turned the early game into a glorified coin toss, data continued to improve each iteration.

<img src="resources/images/blog/14-4.png" style="width: 50%;" />
<div class="blogcaption">pictured: the dreaded Place@17</div>

#### Epochs
I started this project before reading the AlphaGo Zero and AlphaZero papers, and I assumed that I was supposed to generate a batch of training examples, use it once to update the weights, and then discard it. What a waste! This was also before I was distributing my data generation, and I was getting nowhere with this approach. Once I started building large datasets and running multiple epochs, things improved quickly.

#### PyTorch, hyperparameters
* PyTorch is nice, though I'll that admit I expected it to do more of the work for me. "Here are the hyperparameters, here's the data, now train!" Yeah, it doesn't work like that.
* Converting board states into tensors ended up being a major bottleneck. The closer you can get your data to tensor format before importing it to Python/PyTorch, the better, it would seem. Using NumPy arrays as an intermediary improved things.
* I experimented with deeper nets, residual connections, learning rate scheduling, and lots of other stuff, but in almost all cases, the simplest option worked best. I didn't actually try convolutional layers, but that maybe seems like overkill on such a small board?
* Stochastic gradient descent worked better than [Adam](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html) or [AdamW](https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html) in all but one attempt, so I just stuck with SGD.
* Overfitting was a problem even with 5+ training examples per parameter, and while I had some success with L2 regularization, no one value was consistent across generations/topologies. The magic bullet: a dropout rate of 20%. Didn't slow down training too much, and bought lots of time for improvement before overfitting inevitably set in. Since I was bottlenecked on self-play anyway, I went with batch sizes of 128 or 256. This also helped with overfitting, at the cost of severe slowdown.

#### Python
Python is just so, so slow. When I use PyTorch in the future, I will absolutely be generating training data elsewhere and bringing it in at the last second. I never could have dreamed that bitwise operations on IntFlags could eat 15% of my CPU time.

#### CPU/GPU
Since I was running MCTS on the CPU, trying to perform self-play inference on the GPU ended up not making much sense. Even running 64 games in parallel and batching them together, a forward pass through such a small net just doesn't take as long as moving tensors between the CPU and GPU. This combined with the aforementioned Python issues meant that generating data was absolutely the bottleneck.

Once I had the data, training the network on the GPU worked great. The limiting factor then became the 8GB VRAM of my GTX 1070, which prevented me from using DataLoader workers.

#### Curriculum learning
When training fresh networks to test out new topologies, I found that starting from the earliest datasets and progressing through to the latest ones resulted in consistently lower loss and less overfitting. Could certainly just be that more data is better, thought after hundreds of epochs on the most recent data, it was surprising to see a lingering difference from earlier training rounds.

<img src="resources/images/blog/14-5.png" />
<div class="blogcaption">270Kv8 vs. 270Kv8 @100K playouts</div>

## Lingering issues

#### Not enough data
As heavily alluded to before, running self-play games in Python was a huge mistake. Even naive C# would have been a big improvement, and if I had been smart enough to do move generation and MCTS all on the GPU like DeepMind did... maybe next time.

#### Policy in UCT
I wasn't able to find a good description of what to actually *do* with policy logits in MCTS. If you softmax the logits for all legal moves, you end up with a policy term < 1, and if there are many possible moves in a given position, the exploration terms are very small. Generally, it seems to go against the spirit of UCT that its exploration term would be dependent on this, so I just plugged the raw logits into e^x and used that as the multiplier. Not very principled, also seems unlikely that policy training would converge to anything.

#### Standard improvements
There are many techniques now considered standard for this kind of thing that I didn't include: tapering temperature to 0 in self-play, applying Dirichlet noise to policy, playout cap randomization, forced playouts / policy target pruning, policy surprise weighting... much like applying optimizations to minimax, you really could go on forever.

#### Noisy value targets
AlphaZero is a brilliant algorithm, but the one thing that stands out as "wrong" to me is that training examples are labeled simply with a value of 1 or -1, depending on whether the current player won or lost from that position. Where's the nuance?! While training, I consistently saw a smooth descent for policy loss (which, to be fair, has lots of signal) and a nasty, thrashing descent for value loss. I've seen [some evidence](https://medium.com/oracledevs/lessons-from-alphazero-part-4-improving-the-training-target-6efba2e71628) for improvement by interpolating between game result and MCTS result.

#### Discrete training
One of the changes from AlphaGo Zero to AlphaZero was to do away with candidates and promotion matches, and to just have a single current net. I can now see why they made that change: once you've made sure the network is consistently improving, you really want to be generating training data using the absolute freshest weights. I mean, if the thing is getting smarter basically every time you backpropagate, why wouldn't you switch to it immediately?

## Conclusions

<img src="resources/images/blog/14-6.png" style="width: 50%;" />
<div class="blogcaption">270Kv8 spots a "win-in-7-ply" sequence @16K playouts</div>

When I first noticed the network stop placing its pieces along the edge at the start of the game, I was amazed. I mean, this thing barely has any conception of how Push Fight even works, and now it can beat me. Sometimes. I'm confident that if I kept training AlphaPush, it would keep improving, especially if I could rustle up enough training data for a larger net. This project is the culmination of months of learning, and years of *intending to learn.* I could hardly be happier with the results! [Here's the code](https://github.com/thquinn/alphapush), by the way.

## References
* [Mastering the Game of Go with Deep Neural Networks and Tree Search](https://www.davidsilver.uk/wp-content/uploads/2020/03/unformatted_final_mastering_go.pdf) by David Silver et al.
* [Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm](https://arxiv.org/abs/1712.01815) by David Silver et al.
* [Accelerating Self-Play Learning in Go](https://arxiv.org/abs/1902.10565) by David J. Wu
* ["Other Methods Implemented in KataGo"](https://github.com/lightvector/KataGo/blob/master/docs/KataGoMethods.md) by David J. Wu et al.
* Andrej Karpathy's ["Building makemore" series on YouTube](https://www.youtube.com/@AndrejKarpathy/videos)
* ["Simple Alpha Zero"](https://suragnair.github.io/posts/alphazero.html) by Surag Nair